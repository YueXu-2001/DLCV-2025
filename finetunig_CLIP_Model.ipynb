{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c30b4875-a235-490b-a89c-7bce517ef8e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer,CLIPProcessor, CLIPModel\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "from torchvision import transforms,datasets, transforms\n",
    "from tqdm import tqdm\n",
    "import json\n",
    "import os\n",
    "import torch\n",
    "from PIL import Image\n",
    "from torch.utils.data._utils.collate import default_collate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c4291d04-25ad-4487-b831-d5d97b87c7e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load CLIP model and processor\n",
    "model = CLIPModel.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "processor = CLIPProcessor.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"openai/clip-vit-base-patch32\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e044fd31-0616-4667-a7fe-a6dc8fae4637",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Classification head\n",
    "to_pil_image = transforms.ToPILImage()\n",
    "def preprocess_images(images): # putting the images together and resizing it (idk if I should change this to make the compatibility on each other rather then all three on the style_label)\n",
    "    image_tensors = []\n",
    "    for key in images.keys():\n",
    "        image = images[key]  # Extract individual image\n",
    "        processed_image = processor(images=image, return_tensors=\"pt\").pixel_values\n",
    "        image_tensors.append(processed_image)\n",
    "    # Concatenate all the preprocessed images if needed\n",
    "    return torch.cat(image_tensors, dim=0)  # Combine images along batch dimension\n",
    "class CLIPFineTuner(nn.Module):\n",
    "    def __init__(self, clip_model, num_styles):\n",
    "        super(CLIPFineTuner, self).__init__()\n",
    "        self.clip_model = clip_model\n",
    "        #self.fc = nn.Linear(clip_model.visual_projection.in_features, 1) #1 linear model \n",
    "        self.fc = nn.Linear(512, 1)\n",
    "        self.style_embeddings = nn.Embedding(num_styles, clip_model.text_projection.in_features)\n",
    "        \n",
    "    def forward(self, images, style_labels):\n",
    "        # Get CLIP image embeddings\n",
    "        image_tensor = preprocess_images(images)\n",
    "        \n",
    "        image_features = self.clip_model.get_image_features(image_tensor)\n",
    "        \n",
    "        # Get style embeddings\n",
    "        #print(\"style_labels\",style_labels)\n",
    "        inputs = tokenizer(style_labels, padding=True, return_tensors=\"pt\")\n",
    "        style_embeddings = self.clip_model.get_text_features(**inputs)\n",
    "        # Repeat each style embedding 5 times to match the 20 images\n",
    "        expanded_style_embeddings = style_embeddings.repeat_interleave(5, dim=0)\n",
    "        #print(\"Image features shape:\", image_features.shape)\n",
    "        #print(\"Style embeddings shape:\", style_embeddings.shape)\n",
    "        # Cosine similarity for compatibility scoring\n",
    "        compatibility_scores = torch.cosine_similarity(image_features, expanded_style_embeddings)\n",
    "        \n",
    "        # Compatibility prediction (classification)\n",
    "        classification_scores = self.fc(image_features).squeeze(-1)\n",
    "        \n",
    "        return classification_scores, compatibility_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ba1f8882-098b-42b6-8c30-751a57296374",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loss function\n",
    "def compute_loss(classification_scores, compatibility_scores, labels, num_images_per_style):\n",
    "    labels_expanded = labels.repeat_interleave(num_images_per_style)\n",
    "    classification_loss = nn.BCEWithLogitsLoss()(classification_scores, labels_expanded.float()) #change this to use cross-entropy \n",
    "    contrastive_loss = nn.MSELoss()(compatibility_scores, labels_expanded.float())\n",
    "    return classification_loss + contrastive_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5bdc869d-f23e-47de-a154-d8c1f4e626cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Training: might need to change this to take into account the different nature of the dataset\n",
    "def train(model, dataloader, optimizer, epochs, device):\n",
    "    model.train()\n",
    "    for epoch in range(epochs):\n",
    "        total_loss = 0\n",
    "        for batch in tqdm(dataloader):\n",
    "            # Get the images and labels from the batch\n",
    "            images = {category: batch[\"images\"][category].to(device) for category in batch[\"images\"]}\n",
    "            #print(images)\n",
    "            descriptions = batch[\"description\"]\n",
    "            \n",
    "            match_labels = batch[\"match\"].to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # Pass the images and style labels through the model\n",
    "            classification_scores, compatibility_scores = model(images, descriptions)\n",
    "\n",
    "            # Compute the loss\n",
    "            loss = compute_loss(classification_scores, compatibility_scores, match_labels, num_images_per_style = 5)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            total_loss += loss.item()\n",
    "            #Add the print of the loss curve \n",
    "\n",
    "        print(f\"Epoch {epoch + 1}/{epochs}, Loss: {total_loss / len(dataloader)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c445bf12-2977-43bb-8594-fb6a18fc186d",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = r\"C:\\Users\\Jlngo\\Deep Learning in Computer Visions\\Project\\dataset.json\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "20fd6f14-1d35-4b0f-bb19-4d77e0c85f85",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(data_dir,'r') as file:\n",
    "    data = json.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4d079bbb-2ad2-4672-93ef-f2c8062689c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'Outfit_Gender': 'female', 'Outfit_Occasion': 'Campus', 'Outfit_Style': 'Casual', 'Items': [{'Image': '10423_1356_32896545877.jpg', 'category': ['Top'], 'subcategory': ['jacket']}, {'Image': '10020_5257_30537531528.jpg', 'category': ['Pants'], 'subcategory': ['casual pants']}, {'Image': '10000_6916_30454716460.jpg', 'category': ['Shoes'], 'subcategory': ['platform shoes']}], 'match': 1}, {'Outfit_Gender': 'female', 'Outfit_Occasion': 'Campus', 'Outfit_Style': 'Casual', 'Items': [{'Image': '10001_9712_31942033801.jpg', 'category': ['Top'], 'subcategory': ['jacket']}, {'Image': '10001_9720_31632093861.jpg', 'category': ['Skirt'], 'subcategory': ['skirt']}, {'Image': '10001_6916_29041483915.jpg', 'category': ['Shoes'], 'subcategory': ['casual shoes']}], 'match': 1}, {'Outfit_Gender': 'male', 'Outfit_Occasion': 'Home', 'Outfit_Style': 'Simple', 'Items': [{'Image': '10007_6916_31756680500.jpg', 'category': ['Top'], 'subcategory': ['sweaters']}, {'Image': '10002_9730_32136565407.jpg', 'category': ['Top'], 'subcategory': ['jacket']}, {'Image': '10002_9736_31416087628.jpg', 'category': ['Pants'], 'subcategory': ['casual pants']}], 'match': 1}, {'Outfit_Gender': 'male', 'Outfit_Occasion': 'Campus', 'Outfit_Style': 'Japanese', 'Items': [{'Image': '10007_6916_31756680500.jpg', 'category': ['Top'], 'subcategory': ['sweaters']}, {'Image': '10003_9737_31564381628.jpg', 'category': ['Pants'], 'subcategory': ['casual pants']}, {'Image': '10042_1356_31160764076.jpg', 'category': ['Shoes'], 'subcategory': ['sports shoes']}], 'match': 1}, {'Outfit_Gender': 'female', 'Outfit_Occasion': 'Dating', 'Outfit_Style': 'Simple', 'Items': [{'Image': '10005_9715_31544242824.jpg', 'category': ['Top'], 'subcategory': ['windbreaker']}, {'Image': '10004_9719_30896715880.jpg', 'category': ['Skirt'], 'subcategory': ['dress']}, {'Image': '10004_6916_31001592845.jpg', 'category': ['Shoes'], 'subcategory': ['casual shoes']}], 'match': 1}, {'Outfit_Gender': 'female', 'Outfit_Occasion': 'Dating', 'Outfit_Style': 'Elegant', 'Items': [{'Image': '10005_9710_31552331874.jpg', 'category': ['Top'], 'subcategory': ['sweatshirt']}, {'Image': '10005_9715_31544242824.jpg', 'category': ['Pants'], 'subcategory': ['jeans']}, {'Image': '10005_6916_24355710859.jpg', 'category': ['Shoes'], 'subcategory': ['casual shoes']}], 'match': 1}, {'Outfit_Gender': 'unisex', 'Outfit_Occasion': 'Party', 'Outfit_Style': 'Simple', 'Items': [{'Image': '10235_6908_30771792170.jpg', 'category': ['Top'], 'subcategory': ['vests']}, {'Image': '10354_9736_30646555284.jpg', 'category': ['Pants'], 'subcategory': ['jeans']}, {'Image': '10006_6908_29429343280.jpg', 'category': ['Shoes'], 'subcategory': ['casual shoes']}], 'match': 1}, {'Outfit_Gender': 'female', 'Outfit_Occasion': 'Dating', 'Outfit_Style': 'Artistic', 'Items': [{'Image': '10015_9719_30682709245.jpg', 'category': ['Top'], 'subcategory': ['sweater']}, {'Image': '10235_6908_30771792170.jpg', 'category': ['Pants'], 'subcategory': ['casual pants']}, {'Image': '10007_6916_31756680500.jpg', 'category': ['Shoes'], 'subcategory': ['casual shoes']}], 'match': 1}, {'Outfit_Gender': 'unisex', 'Outfit_Occasion': 'Dating', 'Outfit_Style': 'Simple', 'Items': [{'Image': '2039_6191_17135518461.jpg', 'category': ['Top'], 'subcategory': ['jacket']}, {'Image': '10008_9732_29357758315.jpg', 'category': ['Top'], 'subcategory': ['sweatshirt']}, {'Image': '10008_6908_29429343280.jpg', 'category': ['Shoes'], 'subcategory': ['casual shoes']}], 'match': 1}, {'Outfit_Gender': 'male', 'Outfit_Occasion': 'Workplace', 'Outfit_Style': 'Simple', 'Items': [{'Image': '10009_1348_31126594581.jpg', 'category': ['Top'], 'subcategory': []}, {'Image': '10009_9736_31504837950.jpg', 'category': ['Pants'], 'subcategory': ['casual pants']}, {'Image': '10009_6912_30541486638.jpg', 'category': ['Shoes'], 'subcategory': ['formal shoes']}], 'match': 1}]\n"
     ]
    }
   ],
   "source": [
    "print(data[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "03a0eaad-faa2-4b35-b0ad-60edb0fe328e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_image_path(image_folder,target_suffix,subfolders):\n",
    "        # Search for the file within each subfolder\n",
    "        for subfolder in subfolders:\n",
    "            folder_path = os.path.join(image_folder, subfolder)\n",
    "            # Loop through all files in the current subfolder\n",
    "            for dirpath, dirnames, filenames in os.walk(folder_path):\n",
    "                for filename in filenames:\n",
    "                    # Check if the filename ends with the target suffix\n",
    "                    if filename.endswith(target_suffix):\n",
    "                        return os.path.join(dirpath, filename)\n",
    "        return None  # Return None if the image is not found"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9ea0d108-02d9-4b75-a10e-37360d23d985",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ClothingCombinationDataset(Dataset):\n",
    "    def __init__(self, json_file_path, image_folder, transform=None):\n",
    "        # Load JSON data\n",
    "        with open(json_file_path, 'r') as f:\n",
    "            data = json.load(f)\n",
    "        # Store data and parameters\n",
    "        self.data = data\n",
    "        self.image_folder = image_folder\n",
    "        self.transform = transform\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    def __getitem__(self, idx):\n",
    "        outfit = self.data[idx]\n",
    "        images = {}\n",
    "        subfolders = [\"JD_MM19_Train\", \"JD_MM19_Validation\", \"JD_MM19_Test\"]\n",
    "        # Load each item in the outfit\n",
    "        for item in outfit['Items']:\n",
    "            category = item['category'][0]  # Main category, e.g., 'Top', 'Pants', 'Shoes'\n",
    "            image_filename = item['Image']\n",
    "            # Find the image path by searching for the target suffix in the subfolders\n",
    "            target_suffix = image_filename.split('_')[-1]\n",
    "            image_path = find_image_path(image_folder, target_suffix, subfolders)\n",
    "            if image_path:\n",
    "                #print(f\"Image found: {image_path}\")\n",
    "                image = Image.open(image_path).convert(\"RGB\")\n",
    "                if transform:\n",
    "                    image = transform(image)\n",
    "                images[category] = image  # Store the transformed image in a dictionary\n",
    "            else:\n",
    "                #print('IMAGE:',target_suffix)\n",
    "                #print(\"Image not found\")\n",
    "                return None \n",
    "        # Ensure all categories are represented, even if empty\n",
    "        all_categories = ['Bags', 'Pants', 'Shoes', 'Skirt', 'Top']  # Add all possible categories here\n",
    "        for category in all_categories:\n",
    "            if category not in images:\n",
    "                images[category] = torch.zeros(3, 224, 224)  # Placeholder for missing images\n",
    "        # Convert labels to tensors\n",
    "        gender_label = outfit['Outfit_Gender']  # e.g., 'female', 'male', 'unisex'\n",
    "        occasion_label = outfit['Outfit_Occasion']  # e.g., 'Campus', 'Home'\n",
    "        style_label = outfit['Outfit_Style']  # e.g., 'Casual', 'Simple', 'Artistic'\n",
    "        description = \"A \" + gender_label + \" outfit for a \" + occasion_label + \" occasion, featuring a \" + style_label +\" style.\"\n",
    "        match_label = torch.tensor(outfit['match'], dtype=torch.long)\n",
    "        \n",
    "        return {\n",
    "            \"images\": images,  # Dictionary with images per category (e.g., {\"Top\": ..., \"Pants\": ..., \"Shoes\": ...})\n",
    "            \"description\": description,\n",
    "            \"match\": match_label\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f35ab7ae-b17f-441d-8249-a23a942c0da3",
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),  # Resize images\n",
    "    transforms.ToTensor(),          # Convert images to tensors\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8442cd41-a625-4388-8751-f2d32b809844",
   "metadata": {},
   "outputs": [],
   "source": [
    "json_file_path = r\"C:\\Users\\Jlngo\\Deep Learning in Computer Visions\\Project\\dataset.json\"\n",
    "image_folder = r\"C:\\Users\\Jlngo\\Deep Learning in Computer Visions\\Project\\images\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "fa2c2e9f-f83e-4e54-b285-949801503da5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate the dataset\n",
    "dataset = ClothingCombinationDataset(json_file_path=json_file_path, image_folder=image_folder, transform=transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "353fd779",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'images': {'Top': tensor([[[1., 1., 1.,  ..., 1., 1., 1.],\n",
      "         [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "         [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "         ...,\n",
      "         [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "         [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "         [1., 1., 1.,  ..., 1., 1., 1.]],\n",
      "\n",
      "        [[1., 1., 1.,  ..., 1., 1., 1.],\n",
      "         [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "         [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "         ...,\n",
      "         [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "         [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "         [1., 1., 1.,  ..., 1., 1., 1.]],\n",
      "\n",
      "        [[1., 1., 1.,  ..., 1., 1., 1.],\n",
      "         [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "         [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "         ...,\n",
      "         [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "         [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "         [1., 1., 1.,  ..., 1., 1., 1.]]]), 'Skirt': tensor([[[1., 1., 1.,  ..., 1., 1., 1.],\n",
      "         [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "         [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "         ...,\n",
      "         [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "         [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "         [1., 1., 1.,  ..., 1., 1., 1.]],\n",
      "\n",
      "        [[1., 1., 1.,  ..., 1., 1., 1.],\n",
      "         [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "         [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "         ...,\n",
      "         [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "         [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "         [1., 1., 1.,  ..., 1., 1., 1.]],\n",
      "\n",
      "        [[1., 1., 1.,  ..., 1., 1., 1.],\n",
      "         [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "         [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "         ...,\n",
      "         [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "         [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "         [1., 1., 1.,  ..., 1., 1., 1.]]]), 'Shoes': tensor([[[1., 1., 1.,  ..., 1., 1., 1.],\n",
      "         [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "         [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "         ...,\n",
      "         [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "         [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "         [1., 1., 1.,  ..., 1., 1., 1.]],\n",
      "\n",
      "        [[1., 1., 1.,  ..., 1., 1., 1.],\n",
      "         [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "         [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "         ...,\n",
      "         [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "         [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "         [1., 1., 1.,  ..., 1., 1., 1.]],\n",
      "\n",
      "        [[1., 1., 1.,  ..., 1., 1., 1.],\n",
      "         [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "         [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "         ...,\n",
      "         [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "         [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "         [1., 1., 1.,  ..., 1., 1., 1.]]]), 'Bags': tensor([[[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         ...,\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.]],\n",
      "\n",
      "        [[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         ...,\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.]],\n",
      "\n",
      "        [[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         ...,\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.]]]), 'Pants': tensor([[[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         ...,\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.]],\n",
      "\n",
      "        [[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         ...,\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.]],\n",
      "\n",
      "        [[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         ...,\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.]]])}, 'description': 'A female outfit for a Campus occasion, featuring a Casual style.', 'match': tensor(1)}\n"
     ]
    }
   ],
   "source": [
    "# Instantiate the dataset\n",
    "print(dataset.__getitem__(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "da8f0586-2ee7-4b24-9424-960ae96ed2ea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "24836"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4b010e84-d07f-47ac-999a-973d3f9917d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_missing_images(json_file_path, image_folder):\n",
    "    # Load JSON data\n",
    "    with open(json_file_path, 'r') as f:\n",
    "        data = json.load(f)\n",
    "\n",
    "    subfolders = [\"JD_MM19_Train\", \"JD_MM19_Validation\", \"JD_MM19_Test\"]\n",
    "    missing_images_count = 0\n",
    "\n",
    "    # Iterate through each outfit\n",
    "    for outfit in data:\n",
    "        for item in outfit['Items']:\n",
    "            image_filename = item['Image']\n",
    "            target_suffix = image_filename.split('_')[-1]\n",
    "            image_path = find_image_path(image_folder, target_suffix, subfolders)\n",
    "            \n",
    "            if image_path is None:\n",
    "                missing_images_count += 1\n",
    "                print(f\"Missing image: {image_filename}\")\n",
    "\n",
    "    print(f\"Total missing images: {missing_images_count}\")\n",
    "    return missing_images_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "fa8e73e3-9a3e-47a0-8648-dbd429fb337e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#missing_images = count_missing_images(json_file_path, image_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d0d9e37f-1161-428c-b8de-dd5ebd46eaff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the split ratio\n",
    "train_size = int(0.8 * len(dataset))  # 80% for training\n",
    "test_size = len(dataset) - train_size  # 20% for testing\n",
    "\n",
    "# Randomly split the dataset into training and test datasets\n",
    "train_dataset, test_dataset = random_split(dataset, [train_size, test_size])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "16772252-fe2e-48f9-afa8-04390a69bd8c",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'Subset' object has no attribute '_getitem_'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[30], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mtrain_dataset\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_getitem_\u001b[49m(\u001b[38;5;241m10\u001b[39m)\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'Subset' object has no attribute '_getitem_'"
     ]
    }
   ],
   "source": [
    "train_dataset._getitem_(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "101fba37-4b12-4343-a68a-96317fd091b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<torch.utils.data.dataset.Subset object at 0x000001DE0913F230>\n"
     ]
    }
   ],
   "source": [
    "print(train_dataset) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "0ce45c5b-8564-4e4c-8b7b-42126b2626b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_collate(batch):\n",
    "    batch = [item for item in batch if item is not None]\n",
    "    return default_collate(batch) if batch else None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "5c4d7270-3c11-4716-8e35-7555c15917e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Jlngo\\AppData\\Local\\Temp\\ipykernel_29892\\455058077.py:3: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  torch.load(train_dataloader,r\"C:\\Users\\Jlngo\\Deep Learning in Computer Visions\\Project\")\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'DataLoader' object has no attribute 'seek'. You can only torch.load from a file that is seekable. Please pre-load the data into a buffer like io.BytesIO and try to load from it instead.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m----------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                 Traceback (most recent call last)",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\torch\\serialization.py:757\u001b[0m, in \u001b[0;36m_check_seekable\u001b[1;34m(f)\u001b[0m\n\u001b[0;32m    756\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 757\u001b[0m     \u001b[43mf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mseek\u001b[49m(f\u001b[38;5;241m.\u001b[39mtell())\n\u001b[0;32m    758\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'DataLoader' object has no attribute 'seek'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mAttributeError\u001b[0m                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[19], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m train_dataloader \u001b[38;5;241m=\u001b[39m DataLoader(train_dataset, batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m4\u001b[39m, shuffle\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, collate_fn\u001b[38;5;241m=\u001b[39mcustom_collate)\n\u001b[0;32m      2\u001b[0m test_dataloader \u001b[38;5;241m=\u001b[39m DataLoader(test_dataset, batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m4\u001b[39m, shuffle\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, collate_fn\u001b[38;5;241m=\u001b[39mcustom_collate)\n\u001b[1;32m----> 3\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_dataloader\u001b[49m\u001b[43m,\u001b[49m\u001b[38;5;124;43mr\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mC:\u001b[39;49m\u001b[38;5;124;43m\\\u001b[39;49m\u001b[38;5;124;43mUsers\u001b[39;49m\u001b[38;5;124;43m\\\u001b[39;49m\u001b[38;5;124;43mJlngo\u001b[39;49m\u001b[38;5;124;43m\\\u001b[39;49m\u001b[38;5;124;43mDeep Learning in Computer Visions\u001b[39;49m\u001b[38;5;124;43m\\\u001b[39;49m\u001b[38;5;124;43mProject\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m      4\u001b[0m torch\u001b[38;5;241m.\u001b[39mload(test_dataloader,\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mC:\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mUsers\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mJlngo\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mDeep Learning in Computer Visions\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mProject\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\torch\\serialization.py:1319\u001b[0m, in \u001b[0;36mload\u001b[1;34m(f, map_location, pickle_module, weights_only, mmap, **pickle_load_args)\u001b[0m\n\u001b[0;32m   1316\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mencoding\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m pickle_load_args\u001b[38;5;241m.\u001b[39mkeys():\n\u001b[0;32m   1317\u001b[0m     pickle_load_args[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mencoding\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m-> 1319\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[43m_open_file_like\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrb\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m opened_file:\n\u001b[0;32m   1320\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m _is_zipfile(opened_file):\n\u001b[0;32m   1321\u001b[0m         \u001b[38;5;66;03m# The zipfile reader is going to advance the current file position.\u001b[39;00m\n\u001b[0;32m   1322\u001b[0m         \u001b[38;5;66;03m# If we want to actually tail call to torch.jit.load, we need to\u001b[39;00m\n\u001b[0;32m   1323\u001b[0m         \u001b[38;5;66;03m# reset back to the original position.\u001b[39;00m\n\u001b[0;32m   1324\u001b[0m         orig_position \u001b[38;5;241m=\u001b[39m opened_file\u001b[38;5;241m.\u001b[39mtell()\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\torch\\serialization.py:664\u001b[0m, in \u001b[0;36m_open_file_like\u001b[1;34m(name_or_buffer, mode)\u001b[0m\n\u001b[0;32m    662\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _open_buffer_writer(name_or_buffer)\n\u001b[0;32m    663\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m mode:\n\u001b[1;32m--> 664\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_open_buffer_reader\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname_or_buffer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    665\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    666\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExpected \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m or \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mw\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m in mode but got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmode\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\torch\\serialization.py:649\u001b[0m, in \u001b[0;36m_open_buffer_reader.__init__\u001b[1;34m(self, buffer)\u001b[0m\n\u001b[0;32m    647\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, buffer):\n\u001b[0;32m    648\u001b[0m     \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(buffer)\n\u001b[1;32m--> 649\u001b[0m     \u001b[43m_check_seekable\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbuffer\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\torch\\serialization.py:760\u001b[0m, in \u001b[0;36m_check_seekable\u001b[1;34m(f)\u001b[0m\n\u001b[0;32m    758\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m    759\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (io\u001b[38;5;241m.\u001b[39mUnsupportedOperation, \u001b[38;5;167;01mAttributeError\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m--> 760\u001b[0m     \u001b[43mraise_err_msg\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mseek\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtell\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43me\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    761\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\torch\\serialization.py:753\u001b[0m, in \u001b[0;36m_check_seekable.<locals>.raise_err_msg\u001b[1;34m(patterns, e)\u001b[0m\n\u001b[0;32m    746\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m p \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mstr\u001b[39m(e):\n\u001b[0;32m    747\u001b[0m         msg \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m    748\u001b[0m             \u001b[38;5;28mstr\u001b[39m(e)\n\u001b[0;32m    749\u001b[0m             \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m. You can only torch.load from a file that is seekable.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    750\u001b[0m             \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m Please pre-load the data into a buffer like io.BytesIO and\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    751\u001b[0m             \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m try to load from it instead.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    752\u001b[0m         )\n\u001b[1;32m--> 753\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(e)(msg)\n\u001b[0;32m    754\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m e\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'DataLoader' object has no attribute 'seek'. You can only torch.load from a file that is seekable. Please pre-load the data into a buffer like io.BytesIO and try to load from it instead."
     ]
    }
   ],
   "source": [
    "train_dataloader = DataLoader(train_dataset, batch_size=4, shuffle=True, collate_fn=custom_collate)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=4, shuffle=True, collate_fn=custom_collate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "74534ff1-cb9f-4223-a983-bc934b5150d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_file_path = r\"C:\\Users\\Jlngo\\Deep Learning in Computer Visions\\Project\\train_dataset.pth\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "24f6fad1-5c80-4ba7-8379-e696631d175f",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(train_dataset, output_file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "4c9668a8-f0fd-47ed-8b83-df7ca8faa627",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_file_path = r\"C:\\Users\\Jlngo\\Deep Learning in Computer Visions\\Project\\test_dataset.pth\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "b7e6be7e-3c99-4558-a7c0-9908cbcfd95d",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(test_dataset, output_file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 322,
   "id": "f0e73452-a233-46dd-9198-9f8f7db38766",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model = CLIPFineTuner(clip_model=model, num_styles=5).to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 323,
   "id": "bc277722-6c48-4d37-b295-04020f02ab2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▍                   | 106/4967 [23:13<17:44:45, 13.14s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m----------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[323], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_dataloader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[307], line 21\u001b[0m, in \u001b[0;36mtrain\u001b[1;34m(model, dataloader, optimizer, epochs, device)\u001b[0m\n\u001b[0;32m     19\u001b[0m \u001b[38;5;66;03m# Compute the loss\u001b[39;00m\n\u001b[0;32m     20\u001b[0m loss \u001b[38;5;241m=\u001b[39m compute_loss(classification_scores, compatibility_scores, match_labels, num_images_per_style \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m5\u001b[39m)\n\u001b[1;32m---> 21\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     22\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[0;32m     24\u001b[0m total_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mitem()\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\torch\\_tensor.py:581\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    571\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    572\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    573\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[0;32m    574\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    579\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[0;32m    580\u001b[0m     )\n\u001b[1;32m--> 581\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    582\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[0;32m    583\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\torch\\autograd\\__init__.py:347\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    342\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[0;32m    344\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[0;32m    345\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[0;32m    346\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[1;32m--> 347\u001b[0m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    348\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    349\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    350\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    351\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    352\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    353\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    354\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    355\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\torch\\autograd\\graph.py:825\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[1;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[0;32m    823\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[0;32m    824\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 825\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[0;32m    826\u001b[0m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[0;32m    827\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[0;32m    828\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    829\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "train(model, train_dataloader, optimizer, epochs=5 , device = device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a37ca4ad-f033-44f5-9880-82b2b9685cdc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
